# Postmodeling

This module helps with the analyses we perform once we finish training models. Choosing the right model for deployment and exploring its predictions and behavior in time is a critical task. Our `postmodeling` module is aimed at helping answer some of these questions by exploring the outcomes and predictions of the model, and going "deeper" into the model behavior. While the current implementation of the module is tightly coupled with the triage infrastructure, the individual analysis pieces are conceptually valid for any ML model. 

We split the postmodeling process into two steps: 
1. Output verification: Verifying whether the triage experiment generated the outputs that we intended to create and whether they look reasonable for further analyses. This includes, summarizing the validation splits, features that were built, missing data, models we built trained, performace over some priority metric across the validation splits, subset performance, and bias metrics.
2. Model deep-dive: Going deeper into (some of the) models and trying to understand what they are learning. This includes, exploring their predictions, predictive performance across different metrics and thresholds, bias audits, crosstabs etc. Typically, we train dozens of model groups (Model type x hyperparameters) and it can be prohibitive to dive deep into all of these. Therefore, it is advisable to use `Audition` to perform some initial model selection and conduct the deeper dive on a handful of model groups.  

![Postmodeling flow](https://dssg.github.io/triage/postmodeling/postmodeling_general_flow.png)

## Output verification

This step produces an experiment summary report like the examples given here --- named `example_experiment_summary_*.ipynb` --- based on the template notebook `experiment_summary_report_template.ipynb`. This document summarizes the following information for a particular model-building experiment:
- Temporal validation splits
- Model types and objects that were built
- Predictors/features that were built and some stats on missingness of each feature
- Performance of all models wrt to a specified priority metric (defaults to `recall@1_pct`)
- If a bias audit was specified, how well the models are performing in terms of bias with regard to a specified priority bias metric and cohort groups (e.g., protected demographic groups) specified in the experiment config (by default, the metric is `tpr_disparity` and shows metric performance for all subgroups in the attributes specified in the `bias_audit`)
- For the best model of each model type (e.g., best Random Forest, best Decision Tree) based on the prioritized performance metric, it shows feature importance values, recall curve comparisons, feature importance comparisons across model pairs. 

### Generating the Experiment Report

There are two ways to generate the experiment report: 
1. Automatically generate the report after each Triage experiment run
2. Generate the report for a existing experiment

#### 1. Generating the report after each Triage experiment run

To generate the experiment report after every Triage experiment run, do the following **before** running your experiment:

1. Copy the `triage_experiment_report_template.ipynb` to your project repo

2. Install `nbcovert` to the same virtual environment where triage is installed for your project

    ```
    $ pip install nbconvert
    ```

3. Copy the following code snippet to your `run.py` (or other `.py` script that calls triage) if you are using the Python interface for triage (currently the automatic report generation is not supported in the triage CLI version).  

    This snippet will create two versions of the report: (1) an `.html` version that is static snapshot of the triage experiment, and (2) an `.ipynb` version that can be extended with further postmodeling analyses (e.g., save predictions, compare lists generated by different models, calculate crosstabs)

    ```python
    def generate_experiment_report():

        # Path to where you save the notebook template in 
        template_path = '/path/to/the/notebook/template'

        # Specify where you will save the executed notebook (recommend not to overwrite the template)
        output_path = 'path/to/the/notebook/output'

        shutil.copyfile(template_path, output_path)

        os.system(f'jupyter nbconvert --execute --inplace --to notebook {output_path}')
        os.system(f'jupyter nbconvert --to html {output_path}')
    ```

4. Call the `generate_experiment_report()` function after the `.run()` function of the `SingleThreadedExperiment` or `MultiCoreExperiment` class.

5. Add the following keys to the experiment configuration `.yaml` file to specify the priority metrics and groups that will be used to report performance: 

    ```yaml
    scoring:
        # Append these key-value pairs to the scoring section
        priority_metric: 'recall@'
        priority_parameter: '1_pct' 
      
    bias_audit:
        ## Append these key-value pairs to the bias_audit section (if a bias audit is performed)
        priority_metric: 'tpr_disparity'

      priority_groups:
        'race':
          - 'African American'
        'gender':
          - 'Female'
    ```

Once the code is run, you will have a printed summary of the general verifications made on your experiment, an `.html` file and an `.ipynb` file with details on all the verifications made by the process. The `.html` can serve as a report, and the `.ipynb` can serve as a starting point to further postmodeling analysis.  

You can find an example in the notebook [example_generate_experiment_summary_report_automatically_after_experiment_run.ipynb](example_generate_experiment_summary_report_automatically_after_experiment_run.ipynb)

#### 2. Generating the report for an existing experiment

If you need to generate the report for an existing experiment, copy the template notebook `triage_experiment_report_template.ipynb` to your project repo and update the following parameters at the top of the notebook: 

```python
# Triage created hash(es) of the experiment(s) you are interested in. 
# It has to be a list (even if single element)
experiment_hashes = [list, of, hashes]

# Model Performance metric and threshold
# These default to 'recall@' and '1_pct'
performance_metric = 'recall@'
threshold = '1_pct'

# Bias metric defaults to tpr_disparity and bias metric values for all groups generated (if bias audit specified in the experiment config)
bias_metric = 'tpr_disparity'
bias_priority_groups = None

"""If you want to specify priority groups you have to add a dictionary in the following form

bias_priority_groups = {
    'race': ['black/african_american', 'hispanic'],
    'gender': ['women'],
    'locale': ['rural']
}
"""
```

You can find an example in the notebook [example_experiment_summary_given_experiment_hashes.ipynb](example_experiment_summary_given_experiment_hashes.ipynb) and [example_experiment_summary_report_specific_experiment_wSubsets.ipynb](example_experiment_summary_report_specific_experiment_wSubsets.ipynb).

## Model Deep-Dive - WIP

After you have verified that the output of the model building process looks reasonable, you can proceed to more deeply explore the outcomes and predictions of a model and its behavior across time and features. 

The model building process usually produces many models, only some of which we want to consider for further exploration. You may want to use a tool like [Audition](https://github.com/dssg/triage/tree/master/src/triage/component/audition) to select a smaller set of models for this analysis step.


### Saving Predictions of Selected Model Groups

It is likely that you did not save predictions for all the models you built --- This is recommended to prevent database bloat as the `predictions` table can grow pretty quickly. Once you identify the handful of models for further exploration, to enable the deeper dive, we need to save the predictions of those models. We can use the `add_predictions.py` utility to do this. This utility accepts a list of model group ids and an experiment hash, and generates & saves test predictions for the relevant models. 

You can use this model either through the CLI or in Python:

**Command Line Inerface**

This module is available on the triage CLI. You can use a config file to provide the inputs to the module. The example of the config is -- `add_predictions_example_config.yaml`

`triage [-d <database_credentials_file>] savepredictions -c <config_file> `

The `<config_file>` is a required argument. If the `<database_credentials_file>` is not provided, the working directory should contain a `database.yaml`.


**Python Interface**

The function can be imported into a python script to add predictions of selected model groups as shown below. 

    from triage.component.postmodeling.utils.add_predictions import add_predictions

    add_predictions(
        db_engine=conn, # The database connection
        model_groups=[1, 2, 3], # List of model groups  
        project_path='path/to/models/and/matrices', # where the models and matrices are stored
        experiment_hashes=[
            'fdb2ee5499b30d53048a7253cc5be36f', 
            'b045af84cd13830f9b43ce5125ecac0b'
        ], # Restricting models (in the above model groups) based on exeriment (optional)
        train_end_times_range={
            'range_start_date': '2015-01-01',
            'range_end_date': '2017-01-01'
        }, # Restricing models based on train end times (optional). Intervals are inclusive and can be open ended. 
        rank_order='worst', # How to break ties
        replace=True # Whether to replace existing predictions
    )


**Functionality that we support:**
 - for each model group and time (each model id), show:
    - PR-k curve
    - feature importance
    - bias
    - list
       - top k list
       - list cross-tab
       - list descriptives
    - error analysis
 - for each model group, compare
    - feature importances
    - list
    - performance
    - bias

This report is more iterative (than the experiment report)

### Generating the Model Analysis Report

- postmodeling_report_example_acdhs_housing.ipynb
- places to change settings
