# Postmodeling

This module deals with the analyses we perform once the model building is finished. While the individual analysis pieces are valid for any ML model that is built, the current implementation of this module is tightly coupled to the triage infrastructure. 

We split the postmodeling process into two steps: 
1. Output verification: to check whether the output of model building looks reasonable (summarizing number of models trained, best average performance observed over validation splits, subset performance, and bias metrics).
2. Model analysis: to go deeper in exploring the outcomes and predictions of a model and its behavior across time and features in order to choose the right model for deployment.

![Postmodeling flow](https://dssg.github.io/triage/postmodeling/triage_postmodeling_flow_documentation.png)

## Output verification

This step produces an experiment report like the example in `example_name.ipynb`, based on the template notebook `triage_experiment_report_template.ipynb`. This document summarizes the following information for a particular model-building experiment:
- Temporal validation splits
- Model types and objects that were built
- Predictors/features that were built and some stats on missingness of each feature
- Performance of all models wrt to a specified priority metric (defaults to `recall@1_pct`)
- If a bias audit was specified, how well the models are performing in terms of bias with regard to a specified priority bias metric and cohort groups (e.g., protected demographic groups) specified in the experiment config (by default, the metric is `tpr_disparity` and shows metric performance for all subgroups in the attributes specified in the `bias_audit`)
- For the best model of each model type (e.g., best Random Forest, best Decision Tree) based on the prioritized performance metric, it shows feature importance values, recall curve comparisons, feature importance comparisons across model pairs. 

### Generating the Experiment Report

There are two ways to generate the experiment report: 
1. Automatically generate the report after each Triage experiment run
2. Generate the report for an existing experiment

#### 1. Generating the report after each Triage experiment run

To generate the experiment report after every Triage experiment run, do the following **before** running your experiment:

1. Copy the `triage_experiment_report_template.ipynb` to your project repo

2. Install `nbcovert` to the same virtual environment where triage is installed for your project

    ```
    $ pip install nbconvert
    ```

3. Copy the following code snippet to your `run.py` (or other `.py` script that calls triage) if you are using the Python interface for triage (currently the automatic report generation is not supported in the triage CLI version).  

    This snippet will create two versions of the report: (1) an `.html` version that is static snapshot of the triage experiment, and (2) an `.ipynb` version that can be extended with further postmodeling analyses (e.g., save predictions, compare lists generated by different models, calculate crosstabs)

    ```python
    def generate_experiment_report():

        # Path to where you save the notebook template in 
        template_path = '/path/to/the/notebook/template'

        # Specify where you will save the executed notebook (recommend not to overwrite the template)
        output_path = 'path/to/the/notebook/output'

        shutil.copyfile(template_path, output_path)

        os.system(f'jupyter nbconvert --execute --to notebook {output_path}')
        os.system(f'jupyter nbconvert --to html {output_path}')
    ```

4. Call the `generate_experiment_report()` function after the `.run()` function of the `SingleThreadedExperiment` or `MultiCoreExperiment` class.

5. Add the following keys to the experiment configuration `.yaml` file to specify the priority metrics and groups that will be used to report performance: 

    ```yaml
    scoring:
        # Append these key-value pairs to the scoring section
        priority_metric: 'recall@'
        priority_parameter: '1_pct' 
      
    bias_audit:
        ## Append these key-value pairs to the bias_audit section (if a bias audit is performed)
        priority_metric: 'tpr_disparity'

      priority_groups:
        'race':
          - 'African American'
        'gender':
          - 'Female'
    ```

Once the code is run, you will have a printed summary of the general verifications made on your experiment, an `.html` file and an `.ipynb` file with details on all the verifications made by the process. The `.html` can serve as a report, and the `.ipynb` can serve as a starting point to further postmodeling analysis.  

![Summary output](https://dssg.github.io/triage/postmodeling/output_summary_verification.png)


#### 2. Generating the report for an existing experiment

If you need to generate the report for an existing experiment, copy the template notebook `triage_experiment_report_template.ipynb` to your project repo and update the following parameters at the top of the notebook: 

```python
# Triage created hash(es) of the experiment(s) you are interested in. 
# It has to be a list (even if single element)
experiment_hashes = [list, of, hashes]

# Model Performance metric and threshold
# These default to 'recall@' and '1_pct'
performance_metric = 'recall@'
threshold = '1_pct'

# Bias metric defaults to tpr_disparity and bias metric values for all groups generated (if bias audit specified in the experiment config)
bias_metric = 'tpr_disparity'
bias_priority_groups = None

"""If you want to specify priority groups you have to add a dictionary in the following form

bias_priority_groups = {
    'race': ['black/african_american', 'hispanic'],
    'gender': ['women'],
    'locale': ['rural']
}
"""
```

## Model analysis

After you have verified that the output of the model building process looks reasonable, you can proceed to more deeply explore the outcomes and predictions of a model and its behavior across time and features. 

The model building process usually produces many models, only some of which we want to consider for further exploration. You may want to use a tool like [Audition](https://github.com/dssg/triage/tree/master/src/triage/component/audition) to select a smaller set of models for this analysis step.

**Functionality that we support:**
 - for each model group and time (each model id), show:
    - PR-k curve
    - feature importance
    - bias
    - list
       - top k list
       - list cross-tab
       - list descriptives
    - error analysis
 - for each model group, compare
    - feature importances
    - list
    - performance
    - bias

This report is more iterative (than the experiment report)

### Generating the Model Analysis Report

- postmodeling_report_example_acdhs_housing.ipynb
- places to change settings



# Saving Predictions of Selected Models Post Experiment

## Util for Saving Predictions of Selected Model Groups

The script: `add_predictions.py`

config file example: `add_predictions_example_config.py`

This utility accepts a list of model group ids and an experiment hash, and generates & saves test predictions for the relevant models. This util is primarily targetted to be used post-audition for the following use-case:
1. A large model grid was used in the experiment with `save_predictions=False`
2. Audition was used to narrow down model_groups of interest & predictions are required for postmodeling

### Usage

**Command Line Inerface**

This util is available with the triage CLI and can be run using the following command

`triage [-d <database_credentials_file>] savepredictions -c <config_file> `

The `<config_file>` is a required argument. If the `<database_credentials_file>` is not provided, the working directory should contain a `database.yaml`.


**Python Interface**

The function can be imported into a python script to add predictions of selected model groups as shown below. 

    from triage.component.postmodeling.utils.add_predictions import add_predictions

    add_predictions(
        db_engine=conn, # The database connection
        model_groups=[1, 2, 3], # List of model groups  
        project_path='path/to/models/and/matrices', # where the models and matrices are stored
        experiment_hashes=[
            'fdb2ee5499b30d53048a7253cc5be36f', 
            'b045af84cd13830f9b43ce5125ecac0b'
        ], # Restricting models (in the above model groups) based on exeriment (optional)
        train_end_times_range={
            'range_start_date': '2015-01-01',
            'range_end_date': '2017-01-01'
        }, # Restricing models based on train end times (optional). Intervals are inclusive and can be open ended. 
        rank_order='worst', # How to break ties
        replace=True # Whether to replace existing predictions
    )


